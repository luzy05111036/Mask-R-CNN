{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdayvV7CO8af"
      },
      "source": [
        "Install TensorFlow Version 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sskdg804O3QD"
      },
      "source": [
        "!pip install 'tensorflow==1.15.3'\r\n",
        "!pip install 'tensorflow-gpu==1.15.3'\r\n",
        "!pip install 'keras==2.2.4'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV3BQRjv9fdd"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv6b0LrJPH_6"
      },
      "source": [
        "Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTbPCjd391wo"
      },
      "source": [
        "import os\n",
        "from os import listdir\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import skimage.io\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"/content/drive/MyDrive/Project/Mask_RCNN-master/Mask_RCNN-master/\")\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "\n",
        "from mrcnn import config\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.utils import Dataset\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6VNmVsgRHW0"
      },
      "source": [
        "Save some Variables that will be used later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDyqiylBAVl8"
      },
      "source": [
        "Num_Classes = 5\n",
        "#Weight_File = '/content/drive/MyDrive/Project/Mask_RCNN-master/Mask_RCNN-master/logs/final_model20201209T2024/mask_rcnn_final_model_0015.h5'\n",
        "Train_Annot_File = \"/content/drive/MyDrive/Project/Sets/NewTraining/new_annotation.json\"\n",
        "Train_Annot_Image_Dir = \"/content/drive/MyDrive/Project/Sets/NewTraining\"\n",
        "Val_Annot_File = \"/content/drive/MyDrive/Project/Sets/Training/new_annotation.json\"\n",
        "Val_Annot_Image_Dir = \"/content/drive/MyDrive/Project/Sets/Training\"\n",
        "Model_Name = \"Final_Model\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkMR76luRPBt"
      },
      "source": [
        "Override Config for Custom Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGB3Thv1C-VN"
      },
      "source": [
        "class TrainConfig(Config):\n",
        "    # define the name of the configuration\n",
        "    NAME = Model_Name\n",
        "    # number of classes (background + others)\n",
        "    NUM_CLASSES = 1 + 5\n",
        "    # number of training steps per epoch\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    STEPS_PER_EPOCH=2000\n",
        "    \n",
        "    \n",
        "    ###################################################################################\n",
        "    BACKBONE = \"resnet50\"\n",
        "    IMAGES_PER_GPU = 1\n",
        "    \n",
        "    # Length of square anchor side in pixels\n",
        "    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)\n",
        "    \n",
        "    # Non-max suppression threshold to filter RPN proposals.\n",
        "    # You can increase this during training to generate more propsals.\n",
        "    RPN_NMS_THRESHOLD = 0.6\n",
        "    \n",
        "    # How many anchors per image to use for RPN training\n",
        "    RPN_TRAIN_ANCHORS_PER_IMAGE = 512\n",
        "    \n",
        "    # Percent of positive ROIs used to train classifier/mask heads\n",
        "    ROI_POSITIVE_RATIO = 0.6\n",
        "    \n",
        "#     IMAGE_MIN_DIM = 512\n",
        "#     IMAGE_MAX_DIM = 1024\n",
        "    \n",
        "    # Number of ROIs per image to feed to classifier/mask heads\n",
        "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
        "    # enough positive proposals to fill this and keep a positive:negative\n",
        "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
        "    # the RPN NMS threshold.\n",
        "    #TRAIN_ROIS_PER_IMAGE = 512\n",
        "    \n",
        "    # Max number of final detections\n",
        "    DETECTION_MAX_INSTANCES = 100\n",
        "    \n",
        "    # Minimum probability value to accept a detected instance\n",
        "    # ROIs below this threshold are skipped\n",
        "    DETECTION_MIN_CONFIDENCE = 0.5\n",
        "\n",
        "    # Non-maximum suppression threshold for detection\n",
        "    DETECTION_NMS_THRESHOLD = 0.3\n",
        "    pass\n",
        " \n",
        "# prepare config\n",
        "config = TrainConfig()\n",
        "config.display()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n-vKpU7Rv_N"
      },
      "source": [
        "Create Functions to Load the Dataset and Load the Masks. This code was given by the balloon.py implementation of Mask RCNN and was adapted for our Custom Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3ryVaJ9GEYf"
      },
      "source": [
        "\n",
        "class Train_Dataset(utils.Dataset):\n",
        "\n",
        "    def load_dataset(self, dataset_dir):\n",
        "        \"\"\"Load the dataset.\n",
        "        \"\"\"\n",
        "        # Add classes\n",
        "        self.add_class(\"dataset\", 1, \"Head\")\n",
        "        self.add_class(\"dataset\", 2, \"Thread\")\n",
        "        self.add_class(\"dataset\", 3, \"Nut\")\n",
        "        self.add_class(\"dataset\", 4, \"Washer\")\n",
        "        self.add_class(\"dataset\", 5, \"Pin\")\n",
        "\n",
        "        # Load annotations\n",
        "        # VGG Image Annotator (up to version 1.6) saves each image in the form:\n",
        "        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n",
        "        #   'regions': {\n",
        "        #       '0': {\n",
        "        #           'region_attributes': {},\n",
        "        #           'shape_attributes': {\n",
        "        #               'all_points_x': [...],\n",
        "        #               'all_points_y': [...],\n",
        "        #               'name': 'polygon'}},\n",
        "        #       ... more regions ...\n",
        "        #   },\n",
        "        #   'size': 100202\n",
        "        # }\n",
        "        # We mostly care about the x and y coordinates of each region\n",
        "        # Note: In VIA 2.0, regions was changed from a dict to a list.\n",
        "        annotations = json.load(open(os.path.join(dataset_dir, \"new_annotation.json\")))\n",
        "        annotations = list(annotations.values())  # don't need the dict keys\n",
        "\n",
        "        # The VIA tool saves images in the JSON even if they don't have any\n",
        "        # annotations. Skip unannotated images.\n",
        "        annotations = [a for a in annotations if a['regions']]\n",
        "      \n",
        "        # Add images\n",
        "        for a in annotations:\n",
        "            \n",
        "            # Get the x, y coordinaets of points of the polygons that make up\n",
        "            # the outline of each object instance. These are stores in the\n",
        "            # shape_attributes \n",
        "            \n",
        "            polygons = [r['shape_attributes'] for r in a['regions']] \n",
        "            category = [c['region_attributes']['class'] for c in a['regions']]\n",
        "            classes = {\"Head\":1,\"Thread\":2,\"Nut\":3,\"Washer\":4,\"Pin\":5}    \n",
        "            num_ids = [classes[a] for a in category]\n",
        "            \n",
        "            \n",
        "\n",
        "            # load_mask() needs the image size to convert polygons to masks.\n",
        "            # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
        "            # the image. This is only managable since the dataset is tiny.\n",
        "            image_path = os.path.join(dataset_dir, a['filename'])\n",
        "            image = skimage.io.imread(image_path)\n",
        "            height, width = image.shape[:2]\n",
        "\n",
        "            self.add_image(\n",
        "                \"dataset\",\n",
        "                image_id=a['filename'],  # use file name as a unique image id\n",
        "                path=image_path,\n",
        "                width=width, height=height,\n",
        "                polygons=polygons,\n",
        "                num_ids=num_ids)\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Generate instance masks for an image.\n",
        "       Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # If not a dataset image, delegate to parent class.\n",
        "        image_info = self.image_info[image_id]\n",
        "        if image_info[\"source\"] != \"dataset\":\n",
        "            return super(self.__class__, self).load_mask(image_id)\n",
        "\n",
        "        # Convert polygons to a bitmap mask of shape\n",
        "        # [height, width, instance_count]\n",
        "        info = self.image_info[image_id]\n",
        "        num_ids = info['num_ids']\n",
        "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
        "                        dtype=np.uint8)\n",
        "        for i, p in enumerate(info[\"polygons\"]):\n",
        "            # Get indexes of pixels inside the polygon and set them to 1\n",
        "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
        "            mask[rr, cc, i] = 1\n",
        "\n",
        "        # Return mask, and array of class IDs of each instance. Since we have\n",
        "        \n",
        "        return mask.astype(np.bool), np.array(num_ids, dtype=np.int32)\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return the path of the image.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"dataset\":\n",
        "            return info[\"path\"]\n",
        "        else:\n",
        "            super(self.__class__, self).image_reference(image_id)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGIv91rnUQAa"
      },
      "source": [
        "Load the Training and Validation Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXA_OdygL2Ve"
      },
      "source": [
        "# Training dataset\n",
        "dataset_train = Train_Dataset()\n",
        "dataset_train.load_dataset(Train_Annot_Image_Dir)\n",
        "dataset_train.prepare()\n",
        "# Validation dataset\n",
        "dataset_val = Train_Dataset()\n",
        "dataset_val.load_dataset(Val_Annot_Image_Dir)\n",
        "dataset_val.prepare()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doB_EyAuNCJs"
      },
      "source": [
        "Create a Model to Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBXRvsppMm0f"
      },
      "source": [
        "model = modellib.MaskRCNN(mode = 'training', config = TrainConfig(), model_dir = MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35wWnX8pWoMx"
      },
      "source": [
        "Train model: 2000 Steps per Epochs with 15 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9reozL3lwBuR"
      },
      "source": [
        "start_train = time.time()\n",
        "model.train(dataset_train,dataset_val,learning_rate= TrainConfig().LEARNING_RATE, epochs = 15, layers = 'heads')\n",
        "end_train = time.time()\n",
        "minutes = round((end_train - start_train) / 60, 2)\n",
        "print(minutes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1yIZNUEW5eU"
      },
      "source": [
        "Use this to Manually Save Weights "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV1_T6pwDb_7"
      },
      "source": [
        "#model_path = os.path.join(MODEL_DIR, \"mask_rcnn_weights_v6.h5\")\n",
        "#model.keras_model.save_weights(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}