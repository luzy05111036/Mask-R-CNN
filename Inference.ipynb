{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Naoc5oxnZC1p"
      },
      "source": [
        "Install TensorFlow Version 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDnjbINgZAfw"
      },
      "source": [
        "!pip install 'tensorflow==1.6.0'\r\n",
        "!pip install 'keras==2.2.4'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cpqZyjkZBdv"
      },
      "source": [
        "Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3An7nWUemuAd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdywyoRxZKSE"
      },
      "source": [
        "Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aFyaAaMnNkh"
      },
      "source": [
        "import os\n",
        "from os import listdir\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import skimage.io\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"/content/drive/MyDrive/Project/Mask_RCNN-master/Mask_RCNN-master/\")\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "from mrcnn.utils import Dataset\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import utils,visualize\n",
        "from mrcnn.model import log\n",
        "from mrcnn.model import load_image_gt\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.model import mold_image\n",
        "from mrcnn.utils import compute_ap\n",
        "from mrcnn.utils import extract_bboxes\n",
        "\n",
        "#ROOT_DIR2 = os.path.abspath(\"/content/drive/MyDrive/Project/Mask_RCNN-master/Mask_RCNN-master/samples\")\n",
        "#sys.path.append(ROOT_DIR2)\n",
        "#from coco import coco"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy4ZlYWoZM6d"
      },
      "source": [
        "Save some Variables that will be used later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTiykh0Foz_-"
      },
      "source": [
        "Num_Classes = 5\n",
        "Weight_File = '/content/drive/MyDrive/Project/Mask_RCNN-master/Mask_RCNN-master/logs/final_model20201209T2024/mask_rcnn_final_model_0010.h5'\n",
        "#Train_Annot_File = \"/content/drive/MyDrive/Project/Sets/Testing/test_coco.json\"\n",
        "#Train_Annot_Image_Dir = \"/content/drive/MyDrive/Project/Sets/Testing\"\n",
        "Test_Image_Dir = \"/content/drive/MyDrive/Project/Sets/Testing\"\n",
        "Model_Name = \"test_model\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DLJssAtpkK0"
      },
      "source": [
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzr5gKYjZTEG"
      },
      "source": [
        "Override Config for Custom Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8xroqsKp5UL"
      },
      "source": [
        "class TestConfig(Config):\n",
        "    # define the name of the configuration\n",
        "    NAME = 'drone_cfg'\n",
        "    # number of classes (background + others)\n",
        "    NUM_CLASSES = 1 + 5\n",
        "    # number of training steps per epoch\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    STEPS_PER_EPOCH=150\n",
        "    \n",
        "    \n",
        "    ###################################################################################\n",
        "    BACKBONE = \"resnet50\"\n",
        "    IMAGES_PER_GPU = 1\n",
        "    \n",
        "    # Length of square anchor side in pixels\n",
        "    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)\n",
        "    \n",
        "    # Non-max suppression threshold to filter RPN proposals.\n",
        "    # You can increase this during training to generate more propsals.\n",
        "    RPN_NMS_THRESHOLD = 0.6\n",
        "    \n",
        "    # How many anchors per image to use for RPN training\n",
        "    RPN_TRAIN_ANCHORS_PER_IMAGE = 512\n",
        "    \n",
        "    # Percent of positive ROIs used to train classifier/mask heads\n",
        "    ROI_POSITIVE_RATIO = 0.6\n",
        "    \n",
        "#     IMAGE_MIN_DIM = 512\n",
        "#     IMAGE_MAX_DIM = 1024\n",
        "    \n",
        "    # Number of ROIs per image to feed to classifier/mask heads\n",
        "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
        "    # enough positive proposals to fill this and keep a positive:negative\n",
        "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
        "    # the RPN NMS threshold.\n",
        "    #TRAIN_ROIS_PER_IMAGE = 512\n",
        "    \n",
        "    # Max number of final detections\n",
        "    DETECTION_MAX_INSTANCES = 100\n",
        "    \n",
        "    # Minimum probability value to accept a detected instance\n",
        "    # ROIs below this threshold are skipped\n",
        "    DETECTION_MIN_CONFIDENCE = 0.8\n",
        "\n",
        "    # Non-maximum suppression threshold for detection\n",
        "    DETECTION_NMS_THRESHOLD = 0.3\n",
        "    pass\n",
        " \n",
        "# prepare config\n",
        "config = TestConfig()\n",
        "config.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W6_vKmz03Fa"
      },
      "source": [
        "Create the load_dataset and load_mask Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDjhvgCi02WI"
      },
      "source": [
        "class Test_Dataset(utils.Dataset):\r\n",
        "\r\n",
        "    def load_dataset(self, dataset_dir):\r\n",
        "        \"\"\"Load the dataset.\r\n",
        "        \"\"\"\r\n",
        "        # Add classes\r\n",
        "        self.add_class(\"dataset\", 1, \"Head\")\r\n",
        "        self.add_class(\"dataset\", 2, \"Thread\")\r\n",
        "        self.add_class(\"dataset\", 3, \"Nut\")\r\n",
        "        self.add_class(\"dataset\", 4, \"Washer\")\r\n",
        "        self.add_class(\"dataset\", 5, \"Pin\")\r\n",
        "\r\n",
        "        # Load annotations\r\n",
        "        # VGG Image Annotator (up to version 1.6) saves each image in the form:\r\n",
        "        # { 'filename': '28503151_5b5b7ec140_b.jpg',\r\n",
        "        #   'regions': {\r\n",
        "        #       '0': {\r\n",
        "        #           'region_attributes': {},\r\n",
        "        #           'shape_attributes': {\r\n",
        "        #               'all_points_x': [...],\r\n",
        "        #               'all_points_y': [...],\r\n",
        "        #               'name': 'polygon'}},\r\n",
        "        #       ... more regions ...\r\n",
        "        #   },\r\n",
        "        #   'size': 100202\r\n",
        "        # }\r\n",
        "        # We mostly care about the x and y coordinates of each region\r\n",
        "        # Note: In VIA 2.0, regions was changed from a dict to a list.\r\n",
        "        annotations = json.load(open(os.path.join('/content/drive/MyDrive/Project/Sets/', \"new_annotation.json\")))\r\n",
        "        annotations = list(annotations.values())  # don't need the dict keys\r\n",
        "\r\n",
        "        # The VIA tool saves images in the JSON even if they don't have any\r\n",
        "        # annotations. Skip unannotated images.\r\n",
        "        annotations = [a for a in annotations if a['regions']]\r\n",
        "      \r\n",
        "        # Add images\r\n",
        "        for a in annotations:\r\n",
        "            \r\n",
        "            # Get the x, y coordinaets of points of the polygons that make up\r\n",
        "            # the outline of each object instance. These are stores in the\r\n",
        "            # shape_attributes \r\n",
        "            \r\n",
        "            polygons = [r['shape_attributes'] for r in a['regions']] \r\n",
        "            category = [c['region_attributes']['class'] for c in a['regions']]\r\n",
        "            classes = {\"Head\":1,\"Thread\":2,\"Nut\":3,\"Washer\":4,\"Pin\":5}    \r\n",
        "            num_ids = [classes[a] for a in category]\r\n",
        "            \r\n",
        "            \r\n",
        "\r\n",
        "            # load_mask() needs the image size to convert polygons to masks.\r\n",
        "            # Unfortunately, VIA doesn't include it in JSON, so we must read\r\n",
        "            # the image. This is only managable since the dataset is tiny.\r\n",
        "            image_path = os.path.join(dataset_dir, a['filename'])\r\n",
        "            image = skimage.io.imread(image_path)\r\n",
        "            height, width = image.shape[:2]\r\n",
        "\r\n",
        "            self.add_image(\r\n",
        "                \"dataset\",\r\n",
        "                image_id=a['filename'],  # use file name as a unique image id\r\n",
        "                path=image_path,\r\n",
        "                width=width, height=height,\r\n",
        "                polygons=polygons,\r\n",
        "                num_ids=num_ids)\r\n",
        "\r\n",
        "    def load_mask(self, image_id):\r\n",
        "        \"\"\"Generate instance masks for an image.\r\n",
        "       Returns:\r\n",
        "        masks: A bool array of shape [height, width, instance count] with\r\n",
        "            one mask per instance.\r\n",
        "        class_ids: array of class IDs of the instance masks.\r\n",
        "        \"\"\"\r\n",
        "        # If not a dataset image, delegate to parent class.\r\n",
        "        image_info = self.image_info[image_id]\r\n",
        "        if image_info[\"source\"] != \"dataset\":\r\n",
        "            return super(self.__class__, self).load_mask(image_id)\r\n",
        "\r\n",
        "        # Convert polygons to a bitmap mask of shape\r\n",
        "        # [height, width, instance_count]\r\n",
        "        info = self.image_info[image_id]\r\n",
        "        num_ids = info['num_ids']\r\n",
        "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\r\n",
        "                        dtype=np.uint8)\r\n",
        "        for i, p in enumerate(info[\"polygons\"]):\r\n",
        "            # Get indexes of pixels inside the polygon and set them to 1\r\n",
        "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\r\n",
        "            mask[rr, cc, i] = 1\r\n",
        "\r\n",
        "        # Return mask, and array of class IDs of each instance. Since we have\r\n",
        "        \r\n",
        "        return mask.astype(np.bool), np.array(num_ids, dtype=np.int32)\r\n",
        "\r\n",
        "    def image_reference(self, image_id):\r\n",
        "        \"\"\"Return the path of the image.\"\"\"\r\n",
        "        info = self.image_info[image_id]\r\n",
        "        if info[\"source\"] == \"dataset\":\r\n",
        "            return info[\"path\"]\r\n",
        "        else:\r\n",
        "            super(self.__class__, self).image_reference(image_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjRZSJkn1Cvf"
      },
      "source": [
        "Create a Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voqRlBYX1DDN"
      },
      "source": [
        "test_set = Test_Dataset()\r\n",
        "test_set.load_dataset(Test_Image_Dir)\r\n",
        "test_set.prepare()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6hh4cB9ZZE9"
      },
      "source": [
        "Create an Array with the different Class Names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWf3OScsrAQk"
      },
      "source": [
        "class_names = ['BG', 'Head', 'Thread', 'Nut', 'Washer', 'Pin']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwfUJ060ZfvQ"
      },
      "source": [
        "Create a Model that will inference Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UznC0OhOri8i"
      },
      "source": [
        "model = modellib.MaskRCNN(mode = 'inference', config = TestConfig(), model_dir = MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QXHgJJUZnkd"
      },
      "source": [
        "Upload the Weight File that was created by Training.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7_sUw2CrkBH"
      },
      "source": [
        "model.load_weights(Weight_File, by_name=True,)\n",
        "#model.load_weights(model.find_last(), by_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Athb6PEdZxe6"
      },
      "source": [
        "Display the Image without any Annotations for Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7pWbI1YY1iw"
      },
      "source": [
        "file_names = next(os.walk(Test_Image_Dir))[2]\r\n",
        "image = skimage.io.imread(os.path.join(Test_Image_Dir, 'img_34_5_2.jpg'))\r\n",
        "\r\n",
        "skimage.io.imshow(image)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdAjDOy50lMf"
      },
      "source": [
        "Visualize Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psXS0Ns6y-7t"
      },
      "source": [
        "mask, class_ids = test_set.load_mask(24)\r\n",
        "bbox = extract_bboxes(mask)\r\n",
        "visualize.display_instances(image, bbox, mask, class_ids, test_set.class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF3i2oOeaAXY"
      },
      "source": [
        "Have the Model Detect Objects and Display the Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-0ympXesf0B"
      },
      "source": [
        "#file_names = next(os.walk(Test_Image_Dir))[2]\n",
        "#image = skimage.io.imread(os.path.join(Test_Image_Dir, random.choice(file_names)))\n",
        "image = skimage.io.imread(os.path.join(Test_Image_Dir, 'img_34_5_2.jpg'))\n",
        "results = model.detect([image],verbose=0)\n",
        "r = results[0]\n",
        "#print(r)\n",
        "visualize.display_instances(image, r['rois'],r['masks'], r['class_ids'],class_names,r['scores'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1FDvnm71TLW"
      },
      "source": [
        "Calculate mAP Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj0AXUMUm3mQ"
      },
      "source": [
        "image_ids = np.random.choice(test_set.image_ids, len(test_set.image_ids))\r\n",
        "\r\n",
        "APs = []\r\n",
        "\r\n",
        "for id in range(len(image_ids)):\r\n",
        "  image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\r\n",
        "    load_image_gt(test_set, config, image_ids[id], use_mini_mask=False)\r\n",
        "\r\n",
        "  molded_images = np.expand_dims(mold_image(image,config),0)\r\n",
        "  results = model.detect([image],verbose = 0)\r\n",
        "  r = results[0]\r\n",
        "  AP, precisions, recalls, overlaps =\\\r\n",
        "    compute_ap(gt_bbox, gt_class_id, gt_mask, r['rois'],r['class_ids'],r['scores'],r['masks'])\r\n",
        "  APs.append(AP)\r\n",
        "  print(AP)\r\n",
        "\r\n",
        "print(\"mAP: \", np.mean(APs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHj6IGaCpek4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}